---
title: "First explorations in HDB Resale Price Predictor"
description: "Implementing linear regression via rudimentary machine learning"
date: 2023-05-20T12:00:00Z
image: "/images/posts/05.jpg"
categories: ["learning", "hdb prices"]
authors: ["Kelvin Soh"]
tags: ["linear regression", "machine learning"]
draft: false
---

Before implementing gradient descent, I thought to use more simplistic methods first to
get more practice on working in Python and to better understand the foundations. In essence,
I hope to recreate the results obtained from spreadsheets. The Jupyter notebook I used for
this exploration is available on [Github](https://github.com/kelvinsjk/hdb-resale-price-ai/blob/main/learner-basic.ipynb)
if anyone is interested.

Just like in a spreadsheet, we will be attempting multiple linear regression for the `price`
based on normalized (via standardization) data of `date`, `area`, `year` and `floor`.
Our metric and loss function will be the root mean square error.

## Initial failed attempt

We start with random weights (with the initial biased chosen to be the mean price, which I think is a reasonable
first choice since all other parameters are normalized (so they will average to 0)).

My initial idea was as follows: for each parameter `b_i`,
we modify just one index while keeping the others unchanged, and observe
the direction that results in a lower loss function value.

Our new weights for the next step of iteration will then be a step in the directions observed earlier.
For example, consider the initial weights `[1,1,1,1,1]` (for each of our data column plus a bias).
We observe that `[2,1,1,1,1]` caused the loss to decrease while `[1,2,1,1,1]` caused the loss to increase.
On this observation, we will use new weights of `[2,0,...]` for the next step.

This, unfortunately, did not really work. The final loss sometimes increased at a step, leading to
weights oscillating after a few iteration. On further analysis, such a approach is probably too
rudimentary as there are only 2^5 = 32 possible steps that can be taken in the huge 5-dimensional space.
I modified the approach slightly by taking a random step value (e.g. using `np.random()` instead of step size of
1 like in the example above). It improved the results slightly, but quickly led to oscillating results with
weights nowhere near the ideal (for this exploration we have the "correct solution" from the previous work on the spreadsheet)
and final loss too huge to be acceptable.

## The second approach: "random descent"

For this next approach I decided to take a randomized step during each iteration. In essence, it was adding
my previous weights with `torch.random(5)` (though the bias had a different step size because price is not normalized).
We check if the new loss is lower: if it is, take that step. If not, generate a new random step and check if this step
leads to be lower loss. Safeguards to prevent infinite loops were also taken (eg 100/1000 attempts at a random step before
ending the iterations).

This approach yielded better results. The loss went down from ~50,000 to ~30,000. The number of iterations is in the order of
hundreds of thousands, but it was reasonably fast. And I still take that as a win given we have zero performance
optimization (the gradient part of gradient descent) and the low learning rate (using `torch.random()` meant a step of 0.5 each dimension
on average, and the ideal weight is in the order of tens of thousands), so hundreds of thousands of iterations are to be expected.

It got to a point where we were spending too much time generating random steps that didn't improve the loss, breaking out of the loops
as mentioned above. The loss of ~30,000 is still a bit far away from the ideal ~18,000 as well.

I then added new loops to fine-tune the weights. Taking the approach of the first attempt, I stepped in only one dimension and
optimized each weight manually. At this point, I took larger steps of 100 (knowing we are only moving in one dimension gave
me a bit more confidence to take larger steps without worrying about the interplay between the different parameters).
A quick round of iterations and we got the loss down to ~20,000. Not too bad but hardly ideal given the spreadsheet gave us the
answer immediately.

But after this fine-tuning, going back to another round of "random descent" brought us to a final loss of ~17,500. Interestingly,
this is lower than the root mean square error provided by the Google Sheets. I checked it manually with the weights provided
by this model and that is indeed true, but at an improvement of ~0.8%, nothing too drastic that justifies any further
investigation. That said, we did take quite a lot of iterations and just that bit of manual fine tuning before getting to the results.
And after all, it's the non-linear nature of machine learning that I think will be more interesting to explore!

## The cover image

Generated by [Stable Diffusion Online](https://stablediffusionweb.com/).
I used the following prompts, and got something I like at the
3rd
attempt.

1. Robot child on a bicycle with training wheels (interestingly, this prompt repeatedly errored out, with a message asking for an alternative prompt)
2. Training wheels
3. Robot on a bicycle with training wheels
