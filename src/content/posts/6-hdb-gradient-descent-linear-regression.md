---
title: "Linear Regression via Gradient Descent: HDB Resale Price Predictor"
description: "Implementing linear regression via gradient descent"
date: 2023-05-23T12:00:00Z
image: "/images/posts/06.jpg"
categories: ["learning", "hdb prices"]
authors: ["Kelvin Soh"]
tags:
  [
    "linear regression",
    "machine learning",
    "pytorch",
    "neural net",
    "gradient descent",
  ]
draft: false
---

After the previous post of randomly bashing my way to a linear regression model, I
gave gradient descent a try. While less arbitrary than the previous method, it took quite a bit of fiddling around
with learning rates and epochs to get optimal results.

## Attempt 1: manual gradient descent

> The Jupyter notebooks for this is available
> [on GitHub here](https://github.com/kelvinsjk/hdb-resale-price-ai/blob/main/learner-gradient.ipynb)

For the first attempt, I manually differentiated the loss function of the mean square error
(technically I just differentiated the total square error but they differ by just a constant factor.)

I then used the gradient descent method on each row of our data, and applied
changes to the weights each time. Run through the entire data set for `n` epochs and
we've matched the previous results from spreadsheets and the hacky random step approach.

The concept is simple, but it took a bit of fiddling with the learning rates and the number of epochs.
Too large a learning rate and the loss will increase. I got my first optimal result by taking a small learning
rate, but this took ~600 epochs, which, together with iterating through each of the ~300 rows took a non-insignificant
number of time.

Nevertheless, mission accomplished and I'm ready to move on to using PyTorch. I did come back and tweak the learning
rate the next day and managed to get good results in 6 epochs.

## Attempt 2: gradient descent via PyTorch

> The Jupyter notebooks for this is available
> [on GitHub here](https://github.com/kelvinsjk/hdb-resale-price-ai/blob/main/learner-pytorch.ipynb)

After the first attempt I had expected the second via PyTorch to be a breeze. The setup was simple,
but for some reason the results were a lot more sensitive to learning rates than before. Too big a learning
rate and the loss increases. Too small a learning rate and training was taking forever to get to the optimal result.
It took quite a bit of experimentation, and I settled on a loop that altered the learning rate based on the
loss that could reliably achieve the optimal results in less than 100 epochs.

## Attempt 3: my first neural network

> The Jupyter notebooks for this is available
> [on Github here](https://github.com/kelvinsjk/hdb-resale-price-ai/blob/main/learner-neural.ipynb)

Following [lesson 5 of the fast.ai course](https://course.fast.ai/Lessons/lesson5.html) I also
tried implementing a neural net.

On one hand it's pretty cool that in such a short time span from starting the AI journey
I'm able to get one working, but can't lie and say it wasn't a little disappointing that

- I couldn't get it to beat the results from a mere linear regression (though we did get
  to within 10% of that eventually)
- Things were a bit fiddly in terms of the decision of parameters and values
  in the neural net
- Similar to attempt 2 (but worse) it felt like I'm constantly fighting with the learning rate
  between getting the loss to go down quickly and not increasing the loss when we are nearing the optimal results

I guess I'm a bit relieved to see during the lecture that the lecture example
achieved similar results that did not beat a simple linear model. In fact in this exploration
the HDB data set I used (restricted to 3 room flats in Woodlands) is even smaller than the Titanic
data set example.

Hopeful to learn more about fine-tuning neural nets in the future.

## My main takeaway from implementing the models from scratch

> Learn more about learning rates!

## The cover image

Generated by [Stable Diffusion Online](https://stablediffusionweb.com/).
I used the following prompts, and got something I like at the
4th
attempt.

I can't find the robot in the picture I chose, but I think the angle of the
picture of the water slides gave the closest feel for "gradient descent" for
me so I took it as the cover image.

1. Robot going down the stairs
2. Robot going down a playground slide
3. Robot rolling a cheese wheel down a hill
4. Robot sliding down a water slide
